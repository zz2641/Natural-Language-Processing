---
title: "Assignment 3 - NLP"
author: "Charles Lang"
date: "2/23/2017"
output: html_document
---

## Libraries
```{r}
#Make sure you install and load the following libraries

library(tm)
library(SnowballC)
library(wordcloud)
library(ggplot2)
library(dplyr)
library(tidyr)
library(topicmodels)

#IF USING A MAC PLEASE RUN THIS CODE
Sys.setlocale("LC_ALL", "C")
```

## Import all document files and the list of weeks file
```{r}
#Create a list of all the files
file.list <- list.files(path="class-notes", pattern=".csv",full.names = T)
#Loop over file list importing them and binding them together
D1 <- do.call("rbind", lapply(grep(".csv", file.list, value = TRUE), read.csv, header = TRUE, stringsAsFactors = FALSE))
#D1.1<-lapply(file.list, read.csv,header = TRUE, stringsAsFactors = FALSE)
D2 <- read.csv("week-list.csv", header = TRUE)
```

## Step 1 - Clean the htlm tags from your text
```{r}
D1$Notes2 <- gsub("<.*?>", "", D1$Notes)
D1$Notes2 <- gsub("nbsp", "" , D1$Notes2)
D1$Notes2 <- gsub("nbspnbspnbsp", "" , D1$Notes2) # There's someting wrong with the data itself. Needed to be changed
```

## Step 2 - Process text using the tm package
```{r}
#Convert the data frame to the corpus format that the tm package uses
corpus <- VCorpus(VectorSource(D1$Notes2))
#Remove spaces
corpus <- tm_map(corpus, stripWhitespace,lazy = TRUE)
#Convert to lower case
corpus <- tm_map(corpus, tolower)
#Remove pre-defined stop words ('the', 'a', etc)
corpus <- tm_map(corpus, removeWords, stopwords('english'),lazy = TRUE)
#Convert words to stems ("education" = "edu") for analysis, for more info see  http://tartarus.org/~martin/PorterStemmer/
corpus <- tm_map(corpus, stemDocument,lazy = TRUE)
#Remove numbers
corpus <- tm_map(corpus, removeNumbers,lazy = TRUE)
#remove punctuation
corpus <- tm_map(corpus, removePunctuation,lazy = TRUE)
#Convert to plain text for mapping by wordcloud package
corpus <- tm_map(corpus, PlainTextDocument, lazy = TRUE)

#Convert corpus to a term document matrix - so each word can be analyzed individuallly
tdm.corpus <- TermDocumentMatrix(corpus)

#Note: we won't remove plural words here, plural words in English tend to be highly irregular and difficult to extract reliably
```

## Alternative processing - Code has been altered to account for changes in the tm package
```{r}
#Convert the data frame to the corpus format that the tm package uses
corpus <- Corpus(VectorSource(D1$Notes2))
#Remove spaces
corpus <- tm_map(corpus, stripWhitespace)
#Convert to lower case
corpus <- tm_map(corpus, content_transformer(tolower)) 
#Remove pre-defined stop words ('the', 'a', etc)
corpus <- tm_map(corpus, removeWords, stopwords('english'))
#Convert words to stems ("education" = "edu") for analysis, for more info see  http://tartarus.org/~martin/PorterStemmer/
corpus <- tm_map(corpus, stemDocument)
#Remove numbers
corpus <- tm_map(corpus, removeNumbers, lazy=TRUE)
#remove punctuation
corpus <- tm_map(corpus, removePunctuation, lazy=TRUE)
```

What processing steps have you conducted here? Why is this important? Are there any other steps you should take to process your text before analyzing?

### Comments:
 # So far, we converted the dataframe with original texts into a file of corpus. Corpus is the unit that computers can process, and later perform parsing. In the original files, notes are written in a natural languarage patterns. Thus, the following steps of eliminating spaces, punctuation, and numbers; lowercasing all the letters; removing stopwords,words that do not make any practical sense.files.  
## Step 3 - Find common words
```{r}
#The tm package can do some simple analysis, like find the most common words
findFreqTerms(tdm.corpus, lowfreq=50, highfreq=Inf)
#We can also create a vector of the word frequencies
tdm.corpus.matrix<-as.matrix(tdm.corpus)
word.count <- sort(rowSums(tdm.corpus.matrix), decreasing=TRUE)
word.count <- data.frame(word.count)
```

## Generate a Word Cloud

### ColorBrewer
ColorBrewer is a useful tool to help you choose colors for visualizations that was originally built for cartographers. On the ColorBrewer website (http://colorbrewer2.org/#) you can test different color schemes or see what their preset color schemes look like. This is very useful, especially if you are making images for colorblind individuals. 
```{r}
#Define the colors the cloud will use
col=brewer.pal(4,"Dark2")
#Generate cloud
wordcloud(corpus, min.freq=500, scale=c(5,2),rot.per = 0,
          random.color=F, max.word=25, random.order=F,colors=col)

#rot.per:proportion words with 90 degree rotation
```

## Merge with week list so you have a variable representing weeks for each entry 
```{r}
D3<-left_join(D1,D2,by="Title")

```

### Create a Term Document Matrix
```{r}
# #repeat previous steps to include D3 in the corpus
#Convert the data frame to the corpus format that the tm package uses
corpus2<- VCorpus(VectorSource(D3$Notes2))
#Remove spaces
corpus2<- tm_map(corpus2, stripWhitespace,lazy = TRUE)
#Convert to lower case
corpus2<- tm_map(corpus2, tolower)
#Remove pre-defined stop words ('the', 'a', etc)
corpus2<- tm_map(corpus2, removeWords, stopwords('english'),lazy = TRUE)
#Convert words to stems ("education" = "edu") for analysis, for more info see  http://tartarus.org/~martin/PorterStemmer/
corpus2<- tm_map(corpus2, stemDocument,lazy = TRUE)
#Remove numbers
corpus2<- tm_map(corpus2, removeNumbers,lazy = TRUE)
#remove punctuation
corpus2<- tm_map(corpus2, removePunctuation,lazy = TRUE)
#Convert to plain text for mapping by wordcloud package
corpus2<- tm_map(corpus2, PlainTextDocument, lazy = TRUE)

#Convert corpus to a term document matrix - so each word can be analyzed individuallly
tdm.corpus2<- TermDocumentMatrix(corpus2)

```

# Sentiment Analysis

### Match words in corpus to lexicons of positive & negative words
```{r}
#Upload positive and negative word lexicons
positive <- readLines("positive-words.txt")
negative <- readLines("negative-words.txt")

#Search for matches between each word and the two lexicons
D3$positive <- tm_term_score(tdm.corpus2, positive) #Compute a score based on the number of matching terms.
D3$negative <- tm_term_score(tdm.corpus2, negative)

#Generate an overall pos-neg score for each line
D3$score <- D3$positive - D3$negative

```

## Generate a visualization of the sum of the sentiment score over weeks
```{r}
D4 <- D3 %>% 
   group_by(week) %>% 
   summarize(sentiment_score=sum(score))
 D4 <- na.omit(D4)
 plot(D4)
```

# LDA Topic Modelling

Using the same csv file you have generated the LDA analysis will treat each row of the data frame as a document. Does this make sense for generating topics?'


```{r}
#Term Frequency Inverse Document Frequency # previously we have created a term document matrix
dtm.tfi <- DocumentTermMatrix(corpus2, control = list(weighting = weightTf)) #each row/comment has been divided into pieces/columns

#Remove very uncommon terms (term freq inverse document freq < 0.1)
dtm.tfi <- dtm.tfi[,dtm.tfi$v >= 0.1]

# #as.matrix
# dtm.tfi.matrix<-as.matrix(dtm.tfi)
# freq<-colSums(dtm.tfi.matrix)
# ord<-order(freq,decreasing = T)
# freq[ord]

#Keep non-zero entries
rowTotals <- apply(dtm.tfi , 1, sum) #Find the sum of words in each Document
#rowTotals1<-rowSums(as.matrix(dtm.tfi))
dtm.tfi   <- dtm.tfi[rowTotals> 0, ] #Divide by sum across rows

lda.model = LDA(dtm.tfi, k = 5, seed = 150)
# k:number of topics 
# Each row of the input matrix needs to contain at least one non-zero entry
# the documnenttermatxr should not have empty row

#Which terms are most common in each topic
terms(lda.model)

#Which documents belong to which topic
topics(lda.model)

```

What does an LDA topic represent? 

Comment:It presnets us two major findings:1.The most frequent term in each topic 2. Distribution of the topics 

# Main Task 

Your task is to generate a *single* visualization showing: 

- Sentiment for each week and 
- One important topic for that week

```{r}
# Sentiment for each week

D5 <- data.frame(topics(lda.model))
names(D5) = "topic"
D6 <- select(D3, week,Title)
D6 <-na.omit(D6)
D5$ID <- row.names(D5)
D6$ID <- row.names(D6)
D7 <- full_join(D6, D5, by = "ID") %>% select(-ID)
D7 <- na.omit(D7)


# One important topic for that week
 TopTopic <- function(t) {
    diftopic <- unique(t)
    diftopic[which.max(tabulate(match(t, diftopic)))]
 }
 
 D7 <- D7 %>% 
   group_by(week) %>% 
   summarize(MainTopic = TopTopic(topic))
 D7$MainTopic =  as.character(D7$MainTopic)
 
 D8 <- full_join(D7, D4, by = "week")
 
 ggplot(data = D8, mapping = aes(x = week, y = sentiment_score, color = MainTopic)) +
   geom_point()
```


